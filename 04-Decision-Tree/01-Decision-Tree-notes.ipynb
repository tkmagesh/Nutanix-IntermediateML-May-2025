{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "A **Decision Tree** is a supervised machine learning algorithm used for both classification and regression tasks. It is a tree-like structure where each internal node represents a decision based on an attribute, each branch represents an outcome, and each leaf node represents a class label (for classification) or a continuous value (for regression). \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Constructing the Decision Tree\n",
    "\n",
    "The process of building a Decision Tree involves recursively splitting the dataset into subsets based on feature selection. The key mathematical concepts used in this process include:\n",
    "\n",
    "#### 1.1 Selection of Splitting Attribute (Feature Selection)  \n",
    "To determine the best attribute to split on, we use impurity measures or variance reduction:\n",
    "\n",
    "- **Entropy (for classification)**  \n",
    "  The entropy of a dataset measures its impurity or disorder. It is given by:\n",
    "\n",
    "  $$\n",
    "  H(S) = - \\sum_{i=1}^{c} p_i \\log_2 p_i\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "  - $ p_i $ is the probability of class $ i $,\n",
    "  - $ c $ is the number of classes.\n",
    "\n",
    "- **Gini Index (for classification)**  \n",
    "  The Gini Index measures the probability of misclassification:\n",
    "\n",
    "  $$\n",
    "  Gini(S) = 1 - \\sum_{i=1}^{c} p_i^2\n",
    "  $$\n",
    "\n",
    "- **Variance Reduction (for regression)**  \n",
    "  Instead of entropy or Gini, regression trees use the variance of the target values in a node:\n",
    "\n",
    "  $$\n",
    "  Var(S) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "  $$\n",
    "\n",
    "  where $ \\bar{y} $ is the mean of the target variable.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Splitting Criterion: Information Gain & Gini Gain\n",
    "\n",
    "Once we have impurity measures, we evaluate the effectiveness of a feature split using:\n",
    "\n",
    "- **Information Gain (IG):**\n",
    "  \n",
    "  $$\n",
    "  IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)\n",
    "  $$\n",
    "\n",
    "  - $ S $ is the original dataset.\n",
    "  - $ S_v $ is the subset after splitting by feature $ A $.\n",
    "  - $ H(S) $ and $ H(S_v) $ are the entropies before and after splitting.\n",
    "\n",
    "- **Gini Gain:**  \n",
    "  Similar to information gain, it measures impurity reduction:\n",
    "\n",
    "  $$\n",
    "  GG(S, A) = Gini(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Gini(S_v)\n",
    "  $$\n",
    "\n",
    "For regression, we use **Variance Reduction** instead:\n",
    "\n",
    "$$\n",
    "VR(S, A) = Var(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Var(S_v)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Stopping Criteria & Pruning\n",
    "\n",
    "To prevent overfitting, the tree growth is stopped using:\n",
    "- **Maximum depth restriction**\n",
    "- **Minimum samples per node**\n",
    "- **Pruning Techniques:**\n",
    "  - *Pre-pruning:* Stops early using validation-based heuristics.\n",
    "  - *Post-pruning:* Grows the tree fully and prunes unnecessary branches using **cost-complexity pruning**, which minimizes:\n",
    "\n",
    "    $$\n",
    "    C(T) = R(T) + \\alpha |T|\n",
    "    $$\n",
    "\n",
    "    where:\n",
    "    - $ R(T) $ is the misclassification error,\n",
    "    - $ |T| $ is the number of terminal nodes,\n",
    "    - $ \\alpha $ is a regularization parameter.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works?\n",
    "\n",
    "\n",
    "### Step 1: Understanding the Data\n",
    "Before we build a decision tree, we need **data**. Each row represents an instance (example), and each column represents a feature (attribute).\n",
    "\n",
    "#### Example Dataset (for Classification)\n",
    "| Weather  | Temperature | Humidity | Wind | Play Tennis? |\n",
    "|----------|------------|----------|------|-------------|\n",
    "| Sunny    | Hot        | High     | Weak | No          |\n",
    "| Sunny    | Hot        | High     | Strong | No        |\n",
    "| Overcast | Hot        | High     | Weak | Yes        |\n",
    "| Rainy    | Mild       | High     | Weak | Yes        |\n",
    "| Rainy    | Cool       | Normal   | Weak | Yes        |\n",
    "| Rainy    | Cool       | Normal   | Strong | No       |\n",
    "| Overcast | Cool       | Normal   | Strong | Yes      |\n",
    "| Sunny    | Mild       | High     | Weak | No        |\n",
    "| Sunny    | Cool       | Normal   | Weak | Yes       |\n",
    "| Rainy    | Mild       | Normal   | Weak | Yes       |\n",
    "\n",
    "- The **first four columns** are \"features\" (inputs).\n",
    "- The last column (**Play Tennis?**) is the **target** (output we want to predict).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Choosing the Best Feature for Splitting\n",
    "The tree starts at the **root** and splits based on the most important feature. The importance of a feature is determined using **mathematical criteria** like:\n",
    "\n",
    "- **Entropy & Information Gain** (for classification)\n",
    "- **Gini Index** (for classification)\n",
    "- **Variance Reduction** (for regression)\n",
    "\n",
    "#### Example: Calculating Information Gain\n",
    "Let's assume we want to decide the first split.\n",
    "\n",
    "##### 1. Compute Entropy of the Whole Dataset\n",
    "Entropy measures \"uncertainty\" in data:\n",
    "\n",
    "$$\n",
    "H(S) = - p_{yes} \\log_2 p_{yes} - p_{no} \\log_2 p_{no}\n",
    "$$\n",
    "\n",
    "From our dataset:\n",
    "- There are **5 Yes** and **5 No**, so probabilities are **0.5** each.\n",
    "\n",
    "$$\n",
    "H(S) = - (0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1\n",
    "$$\n",
    "\n",
    "##### 2. Compute Entropy After Splitting by a Feature\n",
    "Suppose we split on **Weather** (Sunny, Overcast, Rainy):\n",
    "\n",
    "- **Sunny:** 2 No, 1 Yes → Entropy = 0.918\n",
    "- **Overcast:** 4 Yes, 0 No → Entropy = 0\n",
    "- **Rainy:** 3 Yes, 1 No → Entropy = 0.811\n",
    "\n",
    "The weighted sum:\n",
    "\n",
    "$$\n",
    "H_{after} = \\frac{3}{10} (0.918) + \\frac{4}{10} (0) + \\frac{3}{10} (0.811) = 0.694\n",
    "$$\n",
    "\n",
    "##### 3. Compute Information Gain\n",
    "$$\n",
    "IG = H(S) - H_{after} = 1 - 0.694 = 0.306\n",
    "$$\n",
    "\n",
    "We repeat this for other features (Temperature, Humidity, Wind) and choose the one with the **highest Information Gain** as the root node.\n",
    "\n",
    "In this case, **Weather** has the highest gain, so we split on it first.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Recursively Build the Tree\n",
    "Now we repeat the process **for each branch**:\n",
    "\n",
    "- **Overcast → Always Yes (Leaf Node)**\n",
    "- **Sunny → Further split by Humidity**\n",
    "- **Rainy → Further split by Wind**\n",
    "\n",
    "This continues **until all nodes are pure** (contain only one class) or we reach a stopping condition (e.g., max depth).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Making Predictions\n",
    "Once the tree is built, it is used for predictions.\n",
    "\n",
    "Example:\n",
    "- **Weather = Sunny, Humidity = High** → Tree says \"No\"\n",
    "- **Weather = Rainy, Wind = Weak** → Tree says \"Yes\"\n",
    "\n",
    "The model follows the path from **root to leaf** to make decisions.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
