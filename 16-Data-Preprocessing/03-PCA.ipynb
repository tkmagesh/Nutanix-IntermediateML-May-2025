{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Principal Component Analysis (PCA)?\n",
    "\n",
    "PCA is a statistical technique that transforms a high-dimensional dataset into a lower-dimensional space by identifying the **principal components**—directions (linear combinations of features) that capture the maximum variance in the data. These components are orthogonal (uncorrelated) and ordered by the amount of variance they explain.\n",
    "\n",
    "- **Key Steps in PCA:**\n",
    "  1. **Standardize the Data:** Center the features (zero mean) and scale them (unit variance) since PCA is sensitive to feature scales.\n",
    "  2. **Compute Covariance Matrix:** Measure how features vary together.\n",
    "  3. **Eigenvalue Decomposition:** Find eigenvectors (principal components) and eigenvalues (variance explained by each component).\n",
    "  4. **Project Data:** Transform the original data onto the top `k` principal components, reducing dimensionality from `n` features to `k`.\n",
    "\n",
    "- **Purpose:**\n",
    "  - Reduce dimensionality to simplify models, decrease computation time, and mitigate overfitting.\n",
    "  - Remove noise and redundant information by focusing on the most significant variance.\n",
    "\n",
    "- **Trade-Off:** PCA discards some information (variance in lower components), which might affect accuracy if important patterns are lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data shape: (353, 10)\n",
      "Sample of training data:\n",
      "          age       sex       bmi        bp        s1        s2        s3  \\\n",
      "17   0.070769  0.050680  0.012117  0.056301  0.034206  0.049416 -0.039719   \n",
      "66  -0.009147  0.050680 -0.018062 -0.033213 -0.020832  0.012152 -0.072854   \n",
      "137  0.005383 -0.044642  0.049840  0.097615 -0.015328 -0.016345 -0.006584   \n",
      "245 -0.027310 -0.044642 -0.035307 -0.029770 -0.056607 -0.058620  0.030232   \n",
      "31  -0.023677 -0.044642 -0.065486 -0.081413 -0.038720 -0.053610  0.059685   \n",
      "\n",
      "           s4        s5        s6  \n",
      "17   0.034309  0.027364 -0.001078  \n",
      "66   0.071210  0.000272  0.019633  \n",
      "137 -0.002592  0.017036 -0.013504  \n",
      "245 -0.039493 -0.049872 -0.129483  \n",
      "31  -0.076395 -0.037129 -0.042499  \n"
     ]
    }
   ],
   "source": [
    "### Example Using Scikit-learn’s `diabetes` Dataset\n",
    "\"\"\" We’ll use the `diabetes` dataset (10 features, 442 samples) to apply Linear Regression before and after PCA, comparing MSE and R². \"\"\"\n",
    "\n",
    "#### Step 1: Load and Prepare the Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = diabetes.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Original training data shape:\", X_train.shape)\n",
    "print(\"Sample of training data:\")\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance without PCA:\n",
      "Mean Squared Error: 2900.19\n",
      "R² Score: 0.45\n"
     ]
    }
   ],
   "source": [
    "#### Step 2: Train Model Without PCA (Baseline)\n",
    "\n",
    "# Train Linear Regression without PCA\n",
    "lr_baseline = LinearRegression()\n",
    "lr_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_baseline = lr_baseline.predict(X_test)\n",
    "mse_baseline = mean_squared_error(y_test, y_pred_baseline)\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(\"Performance without PCA:\")\n",
    "print(f\"Mean Squared Error: {mse_baseline:.2f}\")\n",
    "print(f\"R² Score: {r2_baseline:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape after PCA: (353, 5)\n",
      "Explained variance ratio: [0.39688108 0.1477974  0.12516602 0.10108708 0.06582897]\n",
      "Cumulative explained variance: [0.39688108 0.54467848 0.6698445  0.77093158 0.83676055]\n"
     ]
    }
   ],
   "source": [
    "#### Step 3: Apply PCA\n",
    "\"\"\" We’ll standardize the data (required for PCA) and reduce it to, say, 5 components (half the original features), then retrain the model. \"\"\"\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA (retain 5 components)\n",
    "pca = PCA(n_components=5)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(\"Training data shape after PCA:\", X_train_pca.shape)\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Cumulative explained variance:\", np.cumsum(pca.explained_variance_ratio_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Loadings DataFrame:\n",
      "           age       sex       bmi        bp        s1        s2        s3  \\\n",
      "PC1  0.210113  0.160236  0.312409  0.253470  0.354838  0.354777 -0.273562   \n",
      "PC2  0.113868 -0.403087 -0.123077 -0.045113  0.544183  0.407079  0.565671   \n",
      "PC3  0.423640 -0.107244  0.237920  0.557766 -0.159641 -0.352714  0.290746   \n",
      "PC4  0.487631  0.682857 -0.443322  0.060452  0.066322  0.137428  0.106035   \n",
      "PC5  0.676362 -0.345954 -0.060748 -0.564717 -0.124912 -0.124565 -0.197181   \n",
      "\n",
      "           s4        s5        s6  \n",
      "PC1  0.432013  0.383022  0.329204  \n",
      "PC2 -0.140732 -0.019057 -0.073287  \n",
      "PC3 -0.356359  0.109526  0.260588  \n",
      "PC4 -0.038137 -0.229687 -0.083633  \n",
      "PC5  0.087306  0.138717  0.058215  \n"
     ]
    }
   ],
   "source": [
    "print(pca.n_components_)\n",
    "feature_names = X.columns\n",
    "loadings_df = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=feature_names,\n",
    "    index=[f'PC{i+1}' for i in range(pca.n_components_)]\n",
    ")\n",
    "print(\"Loadings DataFrame:\\n\", loadings_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nutanix_int_ml_kernel",
   "language": "python",
   "name": "nutanix_int_ml_kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
