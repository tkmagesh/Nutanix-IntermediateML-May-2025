{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of unscaled training data:\n",
      "          age       sex       bmi        bp        s1        s2        s3  \\\n",
      "17   0.070769  0.050680  0.012117  0.056301  0.034206  0.049416 -0.039719   \n",
      "66  -0.009147  0.050680 -0.018062 -0.033213 -0.020832  0.012152 -0.072854   \n",
      "137  0.005383 -0.044642  0.049840  0.097615 -0.015328 -0.016345 -0.006584   \n",
      "245 -0.027310 -0.044642 -0.035307 -0.029770 -0.056607 -0.058620  0.030232   \n",
      "31  -0.023677 -0.044642 -0.065486 -0.081413 -0.038720 -0.053610  0.059685   \n",
      "\n",
      "           s4        s5        s6  \n",
      "17   0.034309  0.027364 -0.001078  \n",
      "66   0.071210  0.000272  0.019633  \n",
      "137 -0.002592  0.017036 -0.013504  \n",
      "245 -0.039493 -0.049872 -0.129483  \n",
      "31  -0.076395 -0.037129 -0.042499  \n"
     ]
    }
   ],
   "source": [
    "#### Step 1: Load and Prepare the Data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = diabetes.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Sample of unscaled training data:\")\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of training data with exaggerated scales:\n",
      "          age       sex        bmi        bp        s1        s2        s3  \\\n",
      "17   0.070769  0.050680  12.116851  5.630090  0.034206  0.049416 -0.039719   \n",
      "66  -0.009147  0.050680 -18.061887 -3.321323 -0.020832  0.012152 -0.072854   \n",
      "137  0.005383 -0.044642  49.840274  9.761511 -0.015328 -0.016345 -0.006584   \n",
      "245 -0.027310 -0.044642 -35.306880 -2.977038 -0.056607 -0.058620  0.030232   \n",
      "31  -0.023677 -0.044642 -65.485618 -8.141314 -0.038720 -0.053610  0.059685   \n",
      "\n",
      "           s4        s5        s6  \n",
      "17   0.034309  0.027364 -0.001078  \n",
      "66   0.071210  0.000272  0.019633  \n",
      "137 -0.002592  0.017036 -0.013504  \n",
      "245 -0.039493 -0.049872 -0.129483  \n",
      "31  -0.076395 -0.037129 -0.042499  \n"
     ]
    }
   ],
   "source": [
    "# Step 2: Exaggerate Scale Differences\n",
    "\"\"\" To make the effect more pronounced, let’s artificially scale some features (e.g., multiply `bmi` by 1000 and `bp` by 100). \"\"\"\n",
    "\n",
    "\n",
    "# Exaggerate scales\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_scaled['bmi'] = X_train['bmi'] * 1000\n",
    "X_test_scaled['bmi'] = X_test['bmi'] * 1000\n",
    "X_train_scaled['bp'] = X_train['bp'] * 100\n",
    "X_test_scaled['bp'] = X_test['bp'] * 100\n",
    "\n",
    "print(\"Sample of training data with exaggerated scales:\")\n",
    "print(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance without Scaling:\n",
      "Mean Squared Error: 2900.19\n",
      "R² Score: 0.45\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train Model Without Scaling (Baseline)\n",
    "\n",
    "# Train Linear Regression without scaling\n",
    "lr_unscaled = LinearRegression()\n",
    "lr_unscaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_unscaled = lr_unscaled.predict(X_test_scaled)\n",
    "mse_unscaled = mean_squared_error(y_test, y_pred_unscaled)\n",
    "r2_unscaled = r2_score(y_test, y_pred_unscaled)\n",
    "\n",
    "print(\"Performance without Scaling:\")\n",
    "print(f\"Mean Squared Error: {mse_unscaled:.2f}\")\n",
    "\n",
    "#R² values between 0 and 1 indicate the degree of fit, with higher values suggesting a better fit.\n",
    "print(f\"R² Score: {r2_unscaled:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of standardized training data:\n",
      "        age       sex       bmi        bp        s1        s2        s3  \\\n",
      "0  1.498365  1.061370  0.219902  1.138874  0.728473  1.055893 -0.824451   \n",
      "1 -0.228858  1.061370 -0.419366 -0.710591 -0.424929  0.272425 -1.529791   \n",
      "2  0.085182 -0.942179  1.018987  1.992473 -0.309589 -0.326699 -0.119111   \n",
      "3 -0.621409 -0.942179 -0.784662 -0.639458 -1.174640 -1.215508  0.664600   \n",
      "4 -0.542899 -0.942179 -1.423930 -1.706457 -0.799784 -1.110167  1.291569   \n",
      "\n",
      "         s4        s5        s6  \n",
      "0  0.711038  0.547482 -0.061449  \n",
      "1  1.484286 -0.019757  0.367236  \n",
      "2 -0.062210  0.331237 -0.318660  \n",
      "3 -0.835458 -1.069682 -2.719299  \n",
      "4 -1.608706 -0.802859 -0.918820  \n",
      "Performance with Standard Scaling:\n",
      "Mean Squared Error: 2900.19\n",
      "R² Score: 0.45\n"
     ]
    }
   ],
   "source": [
    "#### Step 4: Apply Scaling (StandardScaler)\n",
    "\"\"\" Now, let’s apply **Standard Scaling** (zero mean, unit variance) and retrain the model. \"\"\"\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_standard = scaler.fit_transform(X_train_scaled)\n",
    "X_test_standard = scaler.transform(X_test_scaled)\n",
    "\n",
    "# Convert back to DataFrame for readability (optional)\n",
    "X_train_standard_df = pd.DataFrame(X_train_standard, columns=X_train.columns)\n",
    "print(\"Sample of standardized training data:\")\n",
    "print(X_train_standard_df.head())\n",
    "\n",
    "# Train Linear Regression with scaled data\n",
    "lr_scaled = LinearRegression()\n",
    "lr_scaled.fit(X_train_standard, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_scaled = lr_scaled.predict(X_test_standard)\n",
    "mse_scaled = mean_squared_error(y_test, y_pred_scaled)\n",
    "r2_scaled = r2_score(y_test, y_pred_scaled)\n",
    "\n",
    "print(\"Performance with Standard Scaling:\")\n",
    "print(f\"Mean Squared Error: {mse_scaled:.2f}\")\n",
    "print(f\"R² Score: {r2_scaled:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7         0.          0.33189655  0.3943662   0.89005236  0.76593625\n",
      "   0.42857143  0.42313117  0.61044614  0.48484848]\n",
      " [ 0.91666667  0.          0.50431034  0.54929577  0.31937173  0.31474104\n",
      "   0.36363636  0.14104372  0.3988557   0.42424242]\n",
      " [ 0.78333333  1.          0.34051724  0.4084507   0.80628272  0.52290837\n",
      "   0.55844156  0.28208745  0.81090947  0.43939394]\n",
      " [ 0.93333333  0.          0.56465517  0.78408451  0.62303665  0.48406375\n",
      "   0.09090909  0.83215797  0.86542174  0.72727273]\n",
      " [ 0.55        1.          0.27586207  0.45070423  0.56020942  0.53685259\n",
      "   0.33766234  0.42313117  0.46575871  0.46969697]\n",
      " [ 0.53333333  0.          0.25862069  0.35211268  0.79057592  0.66733068\n",
      "   0.61038961  0.28208745  0.40317315  0.53030303]\n",
      " [ 0.45        1.          1.0387931   0.52112676  0.52879581  0.4750996\n",
      "   0.28571429  0.42313117  0.61514971  0.62121212]\n",
      " [ 0.38333333  1.          0.5387931   0.54929577  0.83246073  0.65039841\n",
      "   0.36363636  0.42313117  0.77124504  0.72727273]\n",
      " [ 0.06666667  1.         -0.00431034  0.22535211  0.31937173  0.27091633\n",
      "   0.33766234  0.28208745  0.57818807  0.51515152]\n",
      " [ 0.21666667  1.          0.41810345  0.38028169  0.55497382  0.52091633\n",
      "   0.42857143  0.28208745  0.36715925  0.5       ]\n",
      " [ 0.3         0.          0.22413793  0.36619718  0.59162304  0.5\n",
      "   0.55844156  0.19746121  0.38562252  0.36363636]\n",
      " [ 0.51666667  0.          0.125       0.36619718  0.15706806  0.15039841\n",
      "   0.16883117  0.28208745  0.65074239  0.1969697 ]\n",
      " [ 0.08333333  0.          0.11206897  0.35211268  0.20418848  0.19422311\n",
      "   0.50649351  0.          0.12383727  0.3030303 ]\n",
      " [ 0.58333333  1.          0.4137931   0.71830986  0.47120419  0.43227092\n",
      "   0.19480519  0.42313117  0.66523922  0.83333333]\n",
      " [ 0.43333333  1.          0.13362069  0.45070423  0.30890052  0.2749004\n",
      "   0.42857143  0.14104372  0.41988136  0.66666667]\n",
      " [ 0.33333333  1.          0.37931034  0.43661972  0.13612565  0.16832669\n",
      "   0.33766234  0.14104372  0.31064621  0.62121212]\n",
      " [ 0.68333333  1.          0.64224138  0.49295775  0.56020942  0.41832669\n",
      "   0.2987013   0.42313117  0.76822633  0.81818182]\n",
      " [ 0.78333333  1.          0.7112069   0.74647887  0.4921466   0.48705179\n",
      "   0.18181818  0.56417489  0.5983713   0.77272727]\n",
      " [ 0.35        1.          0.46982759  0.74647887 -0.06806283  0.02788845\n",
      "   0.16883117  0.10860367  0.36715925  0.56060606]\n",
      " [ 0.71666667  1.          0.59051724  0.74647887  0.46596859  0.43326693\n",
      "   0.28571429  0.42313117  0.57029029  0.60606061]\n",
      " [ 0.16666667  0.          0.51293103  0.32394366  0.36649215  0.25796813\n",
      "   0.27272727  0.28208745  0.74639334  0.45454545]\n",
      " [ 0.71666667  0.          0.18534483  0.23943662  0.58638743  0.52689243\n",
      "   0.48051948  0.28208745  0.38562252  0.27272727]\n",
      " [ 0.3         0.          0.15948276  0.30985915  0.38743455  0.29581673\n",
      "   0.66233766  0.14104372  0.22952719  0.53030303]\n",
      " [ 0.76666667  1.          0.44827586  0.66197183  0.47643979  0.40537849\n",
      "   0.31168831  0.28208745  0.63782513  0.57575758]\n",
      " [ 0.21666667  0.          0.53448276  0.38028169  0.37696335  0.3436255\n",
      "   0.44155844  0.14104372  0.38109446  0.46969697]\n",
      " [ 0.68333333  1.          0.4137931   0.63380282  0.29842932  0.36354582\n",
      "   0.20779221  0.28208745  0.35754151  0.56060606]\n",
      " [ 0.63333333  1.          0.59482759  0.69014085  0.32984293  0.37151394\n",
      "   0.24675325  0.28208745  0.39008038  0.43939394]\n",
      " [ 0.66666667  1.          0.28448276  0.73239437  0.21989529  0.31474104\n",
      "   0.09090909  0.42313117  0.43974867  0.45454545]\n",
      " [ 0.23333333  0.          0.06034483  0.25352113  0.31937173  0.21812749\n",
      "   0.68831169  0.          0.24999122  0.33333333]\n",
      " [ 0.56666667  1.          0.27155172  0.42253521  0.54450262  0.51992032\n",
      "   0.36363636  0.28208745  0.43585243  0.59090909]\n",
      " [ 0.61666667  0.          0.72844828  0.24408451  0.44502618  0.4940239\n",
      "   0.25974026  0.37235543  0.29934361  0.57575758]\n",
      " [ 0.01666667  0.          0.20689655  0.35211268  0.42408377  0.4312749\n",
      "   0.4025974   0.28208745  0.22243673  0.40909091]\n",
      " [ 0.48333333  1.          0.4137931   0.15492958  0.42408377  0.3874502\n",
      "   0.31168831  0.28208745  0.5594791   0.51515152]\n",
      " [ 0.61666667  1.          0.33189655  0.57746479  0.35078534  0.3625498\n",
      "   0.15584416  0.42313117  0.5983713   0.62121212]\n",
      " [ 0.93333333  0.          0.51724138  0.22535211  0.58638743  0.56075697\n",
      "   0.28571429  0.43018336  0.53389027  0.59090909]\n",
      " [ 0.13333333  1.          0.66810345  0.67605634  0.71204188  0.57270916\n",
      "   0.45454545  0.28208745  0.64217768  0.46969697]\n",
      " [ 0.45        1.          0.23275862  0.35211268  0.37172775  0.36454183\n",
      "   0.28571429  0.28208745  0.50945979  0.60606061]\n",
      " [ 0.48333333  1.          0.22413793  0.38492958  0.53403141  0.50398406\n",
      "   0.31168831  0.36812412  0.52493945  0.60606061]\n",
      " [ 0.8         1.          0.23275862  0.47887324  0.5078534   0.4810757\n",
      "   0.25974026  0.42313117  0.57555548  0.8030303 ]\n",
      " [ 0.58333333  0.          0.125       0.22535211  0.40837696  0.32768924\n",
      "   0.62337662  0.14104372  0.24999122  0.22727273]\n",
      " [ 0.48333333  0.          0.07758621  0.4084507   0.41361257  0.33864542\n",
      "   0.61038961  0.14104372  0.24328688  0.65151515]\n",
      " [ 0.8         1.          0.23706897  0.69478873  0.41361257  0.31772908\n",
      "   0.62337662  0.09873061  0.33746358  0.53030303]\n",
      " [ 0.68333333  1.          0.26724138  0.57746479  0.16230366  0.22410359\n",
      "   0.14285714  0.28208745  0.49657763  0.3030303 ]\n",
      " [ 1.          1.          0.22413793  0.36619718  0.39790576  0.43227092\n",
      "   0.14285714  0.56417489  0.54550879  0.66666667]\n",
      " [ 0.68333333  0.          0.17672414  0.60098592  0.58115183  0.31772908\n",
      "   0.49350649  0.23695346  0.83172453  0.53030303]\n",
      " [ 0.41666667  0.          0.21551724  0.35211268  0.53926702  0.42231076\n",
      "   0.71428571  0.14104372  0.21520587  0.21212121]\n",
      " [ 0.05        0.          0.2112069   0.15492958  0.26701571  0.27988048\n",
      "   0.41558442  0.14104372  0.20025273  0.5       ]\n",
      " [ 0.63333333  0.          0.15517241  0.45070423  0.2460733   0.08167331\n",
      "   0.77922078  0.          0.41580961  0.51515152]\n",
      " [ 0.4         0.          0.01724138  0.35211268  0.27748691  0.25896414\n",
      "   0.50649351  0.09449929  0.16834568  0.33333333]\n",
      " [ 0.21666667  0.          0.36206897  0.33802817  0.38743455  0.29880478\n",
      "   0.4025974   0.28208745  0.60805925  0.3030303 ]\n",
      " [ 0.51666667  0.          0.59051724  0.28169014  0.13612565  0.1374502\n",
      "   0.42857143  0.          0.28761978  0.40909091]\n",
      " [ 0.05        0.          0.07758621  0.18309859  0.34031414  0.33366534\n",
      "   0.41558442  0.14104372  0.30503001  0.21212121]\n",
      " [ 0.61666667  1.          0.42672414  0.32394366  0.38743455  0.51394422\n",
      "   0.12987013  0.56417489  0.11421952  0.28787879]\n",
      " [ 0.58333333  0.          0.19396552  0.3943662   0.38219895  0.31175299\n",
      "   0.54545455  0.14104372  0.36715925  0.51515152]\n",
      " [ 0.25        1.          0.28448276  0.78873239  0.7539267   0.71015936\n",
      "   0.22077922  0.70521862  0.62441644  0.34848485]\n",
      " [ 0.36666667  0.          0.61637931  0.45070423  0.31937173  0.312749\n",
      "   0.44155844  0.14104372  0.24999122  0.27272727]\n",
      " [ 0.38333333  0.          0.17241379  0.32394366  0.53926702  0.48306773\n",
      "   0.49350649  0.28208745  0.35754151  0.54545455]\n",
      " [ 0.66666667  1.          0.60344828  0.54929577  0.2460733   0.25697211\n",
      "   0.20779221  0.28208745  0.562217    0.43939394]\n",
      " [ 0.36666667  0.          0.28448276  0.29577465  0.40314136  0.33167331\n",
      "   0.49350649  0.14104372  0.45112149  0.3030303 ]\n",
      " [ 0.31666667  0.          0.13793103  0.14084507  0.28795812  0.09262948\n",
      "   0.85714286  0.          0.41163256  0.48484848]\n",
      " [ 0.38333333  0.          0.47844828  0.43661972  0.72774869  0.66035857\n",
      "   0.2987013   0.56417489  0.61279792  0.51515152]\n",
      " [ 0.3         0.          0.52155172  0.35211268  0.29319372  0.27091633\n",
      "   0.23376623  0.30324401  0.61514971  0.43939394]\n",
      " [ 0.21666667  1.          0.57327586  0.38028169  0.22513089  0.21215139\n",
      "   0.44155844  0.14104372  0.31619221  0.48484848]\n",
      " [ 0.56666667  1.          0.17672414  0.71830986  0.45549738  0.36653386\n",
      "   0.58441558  0.14104372  0.36715925  0.63636364]\n",
      " [ 0.3         0.          0.09482759  0.29577465  0.39267016  0.41334661\n",
      "   0.20779221  0.42313117  0.51261891  0.45454545]\n",
      " [ 0.83333333  0.          0.43103448  0.71830986  0.64921466  0.50398406\n",
      "   0.38961039  0.28208745  0.7090456   0.28787879]\n",
      " [ 0.25        1.          0.31034483  0.69014085  0.62827225  0.59960159\n",
      "   0.22077922  0.56417489  0.60325038  0.48484848]\n",
      " [ 0.73333333  1.          0.25862069  0.69014085  0.38743455  0.35159363\n",
      "   0.28571429  0.28208745  0.58843764  0.36363636]\n",
      " [ 0.          0.          0.21982759  0.18309859  0.17277487  0.14342629\n",
      "   0.38961039  0.14104372  0.48320404  0.21212121]\n",
      " [ 0.18333333  1.          0.30603448  0.29577465  0.35602094  0.38247012\n",
      "   0.15584416  0.42313117  0.5594791   0.37878788]\n",
      " [ 0.15        1.          0.28448276  0.49295775  0.34031414  0.28884462\n",
      "   0.12987013  0.42313117  0.74477869  0.43939394]\n",
      " [ 0.48333333  1.          0.47844828  0.67605634  0.56544503  0.54780876\n",
      "   0.22077922  0.56417489  0.58334796  0.60606061]\n",
      " [ 0.76666667  0.          0.27155172  0.81690141  0.58638743  0.46812749\n",
      "   0.19480519  0.56417489  0.79023483  1.        ]\n",
      " [ 0.21666667  1.          0.31465517  0.39901408  0.22513089  0.29282869\n",
      "   0.15584416  0.35260931  0.44736565  0.37878788]\n",
      " [ 0.8         0.          0.11206897  0.29577465  0.31413613  0.28984064\n",
      "   0.48051948  0.14104372  0.26933202  0.28787879]\n",
      " [ 0.53333333  1.          0.19827586  0.57746479  0.56020942  0.60159363\n",
      "   0.1038961   0.70521862  0.54550879  0.33333333]\n",
      " [ 0.36666667  1.          0.59913793  0.66197183  0.7382199   0.64243028\n",
      "   0.35064935  0.42313117  0.63115588  0.68181818]\n",
      " [ 0.28333333  0.          0.51293103  0.46478873  0.47643979  0.41633466\n",
      "   0.25974026  0.39351199  0.65702552  0.40909091]\n",
      " [ 0.7         1.          0.43534483  0.61971831  0.4921466   0.4501992\n",
      "   0.38961039  0.28208745  0.47284917  0.57575758]\n",
      " [ 0.55        0.          0.27586207  0.3943662   0.46073298  0.43525896\n",
      "   0.09090909  0.70521862  0.71613605  0.42424242]\n",
      " [ 0.1         0.          0.34051724  0.42253521  0.40314136  0.39243028\n",
      "   0.44155844  0.14104372  0.24999122  0.45454545]\n",
      " [ 0.23333333  0.          0.31034483  0.32394366  0.23560209  0.21613546\n",
      "   0.37662338  0.14104372  0.45484222  0.18181818]\n",
      " [ 0.26666667  0.          0.09913793  0.04225352  0.40314136  0.3187251\n",
      "   0.58441558  0.11142454  0.35754151  0.3030303 ]\n",
      " [ 0.36666667  0.          0.11637931  0.33802817  0.59162304  0.4312749\n",
      "   0.79220779  0.14104372  0.28761978  0.46969697]\n",
      " [ 0.5         0.          0.09482759  0.43661972  0.38743455  0.30577689\n",
      "   0.50649351  0.14104372  0.47284917  0.53030303]\n",
      " [ 0.55        0.          0.25431034  0.29577465  0.29842932  0.22410359\n",
      "   0.63636364  0.          0.2077995   0.54545455]\n",
      " [ 0.36666667  0.          0.10344828  0.25352113  0.07329843  0.03585657\n",
      "   0.54545455  0.          0.26933202  0.25757576]\n",
      " [ 0.38333333  0.          0.07758621  0.1971831   0.18848168  0.20717131\n",
      "   0.42857143  0.14104372  0.14233564  0.31818182]\n",
      " [ 0.2         0.          0.5         0.36619718  0.29842932  0.30776892\n",
      "   0.33766234  0.28208745  0.38562252  0.3030303 ]]\n",
      "Performance with Min-Max Scaling:\n",
      "Mean Squared Error: 2900.19\n",
      "R² Score: 0.45\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Apply Normalization (MinMaxScaler)\n",
    "# Let’s also try **Min-Max Scaling** (to [0, 1]) for comparison.\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Apply MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X_train_minmax = minmax_scaler.fit_transform(X_train_scaled)\n",
    "X_test_minmax = minmax_scaler.transform(X_test_scaled)\n",
    "\n",
    "print(X_test_minmax)\n",
    "\n",
    "# Train Linear Regression with Min-Max scaled data\n",
    "lr_minmax = LinearRegression()\n",
    "lr_minmax.fit(X_train_minmax, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_minmax = lr_minmax.predict(X_test_minmax)\n",
    "mse_minmax = mean_squared_error(y_test, y_pred_minmax)\n",
    "r2_minmax = r2_score(y_test, y_pred_minmax)\n",
    "\n",
    "print(\"Performance with Min-Max Scaling:\")\n",
    "print(f\"Mean Squared Error: {mse_minmax:.2f}\")\n",
    "print(f\"R² Score: {r2_minmax:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nutanix_int_ml_kernel",
   "language": "python",
   "name": "nutanix_int_ml_kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
