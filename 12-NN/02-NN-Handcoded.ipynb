{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.2312\n",
      "Epoch 10: Loss = 0.0806\n",
      "Epoch 20: Loss = 0.0236\n",
      "Epoch 30: Loss = 0.0062\n",
      "Epoch 40: Loss = 0.0015\n",
      "Epoch 50: Loss = 0.0004\n",
      "Epoch 60: Loss = 0.0001\n",
      "Epoch 70: Loss = 0.0000\n",
      "Epoch 80: Loss = 0.0000\n",
      "Epoch 90: Loss = 0.0000\n",
      "\n",
      "Final Weights & Biases:\n",
      "W1: 0.6824, B1: 0.1912\n",
      "W2: -0.3000, B2: 0.2000\n",
      "W3: 0.8554, W4: -0.6000, B3: 0.1683\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input layer (1) = 1 neuron\n",
    "# Hidden layer (1) = 2 neurons\n",
    "# Output layer (1) = 1 neuron\n",
    "\n",
    "# Initialize network parameters (weights & biases)\n",
    "W1, B1 = 0.5, 0.1   # Hidden Neuron 1\n",
    "W2, B2 = -0.3, 0.2  # Hidden Neuron 2\n",
    "W3, W4, B3 = 0.7, -0.6, 0.05  # Output Neuron\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.01  # Learning rate\n",
    "epochs = 100  # Number of training iterations\n",
    "\n",
    "# Training data (single example)\n",
    "X = 2  # Input\n",
    "Y_true = 1.5  # Expected output\n",
    "\n",
    "# Activation function (ReLU)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Derivative of ReLU\n",
    "def relu_derivative(x):\n",
    "    return 1 if x > 0 else 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # ** FORWARD PROPAGATION **\n",
    "    Z1 = (W1 * X) + B1\n",
    "    A1 = relu(Z1)  # Activation for Hidden Neuron 1\n",
    "    \n",
    "    Z2 = (W2 * X) + B2\n",
    "    A2 = relu(Z2)  # Activation for Hidden Neuron 2\n",
    "    \n",
    "    Z_out = (W3 * A1) + (W4 * A2) + B3  # Output neuron (no activation)\n",
    "    Y_pred = Z_out  # Final output\n",
    "    \n",
    "    # Compute loss (Mean Squared Error)\n",
    "    loss = 0.5 * (Y_true - Y_pred) ** 2\n",
    "\n",
    "    # ** BACKPROPAGATION **\n",
    "    dL_dY_pred = Y_pred - Y_true  # Gradient of loss w.r.t output\n",
    "\n",
    "    # Gradients for Output Layer\n",
    "    dL_dW3 = dL_dY_pred * A1\n",
    "    dL_dW4 = dL_dY_pred * A2\n",
    "    dL_dB3 = dL_dY_pred\n",
    "\n",
    "    # Gradients for Hidden Layer\n",
    "    dL_dA1 = dL_dY_pred * W3\n",
    "    dL_dA2 = dL_dY_pred * W4\n",
    "\n",
    "    # Apply ReLU derivative\n",
    "    dL_dZ1 = dL_dA1 * relu_derivative(Z1)\n",
    "    dL_dZ2 = dL_dA2 * relu_derivative(Z2)\n",
    "\n",
    "    # Gradients for Hidden Layer Weights & Biases\n",
    "    dL_dW1 = dL_dZ1 * X\n",
    "    dL_dB1 = dL_dZ1\n",
    "\n",
    "    dL_dW2 = dL_dZ2 * X\n",
    "    dL_dB2 = dL_dZ2\n",
    "\n",
    "    # ** WEIGHT UPDATES (Gradient Descent) **\n",
    "    W1 -= alpha * dL_dW1\n",
    "    W2 -= alpha * dL_dW2\n",
    "    B1 -= alpha * dL_dB1\n",
    "    B2 -= alpha * dL_dB2\n",
    "    W3 -= alpha * dL_dW3\n",
    "    W4 -= alpha * dL_dW4\n",
    "    B3 -= alpha * dL_dB3\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "# Final results\n",
    "print(\"\\nFinal Weights & Biases:\")\n",
    "print(f\"W1: {W1:.4f}, B1: {B1:.4f}\")\n",
    "print(f\"W2: {W2:.4f}, B2: {B2:.4f}\")\n",
    "print(f\"W3: {W3:.4f}, W4: {W4:.4f}, B3: {B3:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Trng_WS",
   "language": "python",
   "name": "ml_trng_ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
