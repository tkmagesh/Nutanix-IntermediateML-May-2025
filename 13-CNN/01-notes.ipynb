{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding Convolutional Neural Networks (CNNs) for Beginners**  \n",
    "\n",
    "A **Convolutional Neural Network (CNN)** is a type of deep learning model specifically designed to process and analyze images. It mimics how the human brain perceives visual information.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Why Do We Need CNNs?**\n",
    "Traditional machine learning models (like logistic regression or fully connected neural networks) struggle with images because:  \n",
    "1. **Too many pixels!** A 100×100 image has **10,000** features per color channel. A normal neural network would have millions of parameters, making it inefficient.  \n",
    "2. **Loses spatial relationships.** Regular neural networks treat pixels as independent numbers and cannot recognize patterns like shapes and edges.  \n",
    "3. **Memory and computation are expensive.** Fully connected layers require too many weights, making training very slow.  \n",
    "\n",
    "CNNs solve these problems by **learning patterns (edges, shapes, textures) from smaller regions of the image**, rather than treating all pixels as separate inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## **How Does a CNN Work?**\n",
    "A CNN has multiple **layers** that transform an image step by step. The most important ones are:\n",
    "\n",
    "1. **Convolution Layer**\n",
    "2. **Activation Function (ReLU)**\n",
    "3. **Pooling Layer**\n",
    "4. **Fully Connected Layer (FC Layer)**\n",
    "5. **Softmax / Sigmoid (for classification)**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Convolution Layer – Feature Extraction**\n",
    "Imagine you're looking at a picture of a cat. Instead of analyzing each pixel, CNNs use small **filters (kernels)** to scan the image and detect patterns like edges, textures, or colors.\n",
    "\n",
    "### **How does convolution work?**\n",
    "A **filter** (e.g., a 3×3 matrix) slides over the image, multiplying values and summing them up to create a new matrix called a **feature map**.  \n",
    "\n",
    " **Example of a 3×3 filter detecting edges:**  \n",
    "\n",
    "#### **Original Image (5×5 grayscale pixels)**\n",
    "```\n",
    "1  2  3  4  5  \n",
    "6  7  8  9 10  \n",
    "11 12 13 14 15  \n",
    "16 17 18 19 20  \n",
    "21 22 23 24 25  \n",
    "```\n",
    "#### **3×3 Filter (Edge Detector)**\n",
    "```\n",
    "-1 -1 -1  \n",
    " 0  0  0  \n",
    " 1  1  1  \n",
    "```\n",
    "When this filter slides over the image, it highlights **horizontal edges**. The CNN learns **multiple filters** to detect different features like edges, textures, and shapes.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Activation Function (ReLU)**\n",
    "After convolution, we apply an activation function like **ReLU (Rectified Linear Unit)** to keep only important features and remove negative values.\n",
    "\n",
    " **ReLU Rule:**  \n",
    "- If the value is **positive**, keep it.  \n",
    "- If the value is **negative**, change it to **0**.  \n",
    "\n",
    "Example:  \n",
    "```\n",
    "Before ReLU:   [-5, 2, -3, 8]  \n",
    "After ReLU:    [ 0, 2,  0, 8]  \n",
    "```\n",
    "This helps the CNN focus on important patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Pooling Layer – Reducing Image Size**\n",
    "Pooling **reduces the size** of feature maps while keeping the important information. It helps:\n",
    "✔️ Reduce computation  \n",
    "✔️ Make the model **faster**  \n",
    "✔️ Handle **small shifts** in the image (translation invariance)  \n",
    "\n",
    "The most common pooling method is **Max Pooling**, where we take the **largest value** in a small region.\n",
    "\n",
    " **Example of 2×2 Max Pooling:**\n",
    "```\n",
    "Before Pooling:  \n",
    "[1  3  2  4]  \n",
    "[5  6  7  8]  \n",
    "[9 10 11 12]  \n",
    "[13 14 15 16]  \n",
    "\n",
    "After 2×2 Max Pooling:  \n",
    "[6  8]  \n",
    "[14 16]  \n",
    "```\n",
    "Now, the image is **smaller**, but the key features remain!\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Fully Connected Layer – Making Predictions**\n",
    "After convolution and pooling, we **flatten** the feature maps into a **1D vector** and connect it to a normal neural network.\n",
    "\n",
    "This layer:\n",
    "- Learns the relationship between detected features and labels  \n",
    "- Uses a softmax function to classify images into categories (e.g., \"cat\" vs. \"dog\")  \n",
    "\n",
    "---\n",
    "\n",
    "## **5. Output Layer – Final Prediction**\n",
    "The final layer depends on the task:\n",
    "- **Classification:** Uses **Softmax** to output probabilities (e.g., 90% cat, 10% dog).  \n",
    "- **Binary Classification:** Uses **Sigmoid** (output between 0 and 1).  \n",
    "- **Regression:** Outputs continuous values (e.g., predicting age from a face image).  \n",
    "\n",
    "---\n",
    "\n",
    "## **Example CNN Architecture for Image Classification**\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define a CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)), # Convolution\n",
    "    layers.MaxPooling2D((2,2)),  # Pooling\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.Flatten(),  # Flatten into 1D\n",
    "    layers.Dense(64, activation='relu'),  # Fully connected layer\n",
    "    layers.Dense(10, activation='softmax')  # Output layer (10 classes)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "| Step | Layer | Purpose |\n",
    "|------|-------|---------|\n",
    "| **1** | **Convolution** | Detect features like edges & textures |\n",
    "| **2** | **ReLU Activation** | Keeps important values (removes negatives) |\n",
    "| **3** | **Pooling** | Reduces image size, keeps key info |\n",
    "| **4** | **Fully Connected Layer** | Combines features for classification |\n",
    "| **5** | **Softmax / Sigmoid** | Predicts output category |\n",
    "\n",
    "---\n",
    "\n",
    "## **Real-World Uses of CNNs**\n",
    "- **Self-Driving Cars** → Detect lanes, pedestrians, traffic signs  \n",
    "- **Medical Diagnosis** → Identify tumors in X-rays  \n",
    "- **Facial Recognition** → Unlock phones  \n",
    "- **Object Detection** → Used in security cameras  \n",
    "- **Art Generation** → Create deepfake images  \n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**\n",
    "✔ **CNNs are specialized for images** and can detect patterns like edges, shapes, and textures.  \n",
    "✔ They use **Convolution** to extract features and **Pooling** to reduce size.  \n",
    "✔ **Fully Connected Layers** predict the final output (e.g., cat/dog).  \n",
    "✔ CNNs are **widely used** in AI applications like self-driving cars, medical imaging, and facial recognition.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding Convolution Step-by-Step with a 5×5 Image and a 3×3 Edge Detector Filter**  \n",
    "\n",
    "#### **What is Convolution?**\n",
    "Convolution is a mathematical operation used in **Convolutional Neural Networks (CNNs)** to extract features (like edges, textures, and patterns) from an image. It involves sliding a small matrix (**kernel/filter**) over the image and computing a new transformed matrix (**feature map**).\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: Define the Input Image (5×5)**\n",
    "Let's assume we have a **grayscale image** represented as a **5×5 pixel matrix** (values between 0 and 255 represent pixel intensity):\n",
    "\n",
    "```\n",
    "10   20   30   40   50\n",
    "60   70   80   90  100\n",
    "110  120  130  140  150\n",
    "160  170  180  190  200\n",
    "210  220  230  240  250\n",
    "```\n",
    "\n",
    "Each number represents the brightness of a pixel.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Define the 3×3 Edge Detector Filter**\n",
    "A **Sobel filter** is commonly used for detecting horizontal or vertical edges. Let's use a **horizontal edge detector filter**:\n",
    "\n",
    "```\n",
    "-1  -1  -1\n",
    " 0   0   0\n",
    " 1   1   1\n",
    "```\n",
    "\n",
    "This filter highlights **horizontal edges** by emphasizing differences in pixel intensities between the top and bottom parts of the image.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: Apply Convolution**\n",
    "We slide the **3×3 filter** over the **5×5 image**, perform element-wise multiplication, sum the results, and store the value in a new matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### **First Step (Top-left corner)**\n",
    "Take the first **3×3 region** from the top-left of the image:\n",
    "\n",
    "```\n",
    "10   20   30  \n",
    "60   70   80  \n",
    "110  120  130  \n",
    "```\n",
    "\n",
    "Multiply with the **edge detector filter** element-wise:\n",
    "\n",
    "```\n",
    "(10 × -1) + (20 × -1) + (30 × -1) +\n",
    "(60 ×  0) + (70 ×  0) + (80 ×  0) +\n",
    "(110 × 1) + (120 × 1) + (130 × 1)\n",
    "```\n",
    "\n",
    "Compute the sum:\n",
    "\n",
    "```\n",
    "(-10) + (-20) + (-30) + (0) + (0) + (0) + (110) + (120) + (130) = 200\n",
    "```\n",
    "\n",
    "Place **200** in the corresponding position of the output feature map.\n",
    "\n",
    "---\n",
    "\n",
    "### **Move the Filter Right**\n",
    "Move the filter **one step (stride = 1)** to the right:\n",
    "\n",
    "```\n",
    "20   30   40  \n",
    "70   80   90  \n",
    "120  130  140  \n",
    "```\n",
    "\n",
    "Perform the same multiplication:\n",
    "\n",
    "```\n",
    "(20 × -1) + (30 × -1) + (40 × -1) +\n",
    "(70 ×  0) + (80 ×  0) + (90 ×  0) +\n",
    "(120 × 1) + (130 × 1) + (140 × 1)\n",
    "```\n",
    "\n",
    "Compute the sum:\n",
    "\n",
    "```\n",
    "(-20) + (-30) + (-40) + (0) + (0) + (0) + (120) + (130) + (140) = 300\n",
    "```\n",
    "\n",
    "Place **300** in the next position.\n",
    "\n",
    "---\n",
    "\n",
    "### **Repeat for Entire Image**\n",
    "After sliding the filter across the entire image, we get a **new feature map (convolved image)**:\n",
    "\n",
    "```\n",
    "200   300   300  \n",
    "300   400   400  \n",
    "300   400   400  \n",
    "```\n",
    "\n",
    "The final **3×3 feature map** is smaller than the original **5×5 image** because the filter cannot go beyond the edges.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4: Stride and Padding**\n",
    "- **Stride (step size):** Default is 1, but if increased (e.g., stride = 2), it skips pixels, reducing the feature map size.\n",
    "- **Padding (adding extra pixels):** \"Same padding\" keeps the original size by adding zeros around the image.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 5: Why Use Convolution?**\n",
    "- Detects edges, textures, and patterns.\n",
    "- Reduces the number of parameters compared to fully connected networks.\n",
    "- Helps in tasks like **image recognition** and **object detection**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Convolution extracts features** from an image using a filter.  \n",
    "- It **slides over the image**, multiplying values and summing them.  \n",
    "- The result is a **feature map** that highlights important structures like edges.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What Are Filters in `Conv2D(32, (3,3), ...)`?**  \n",
    "When we use `Conv2D(32, (3,3), ...)`, the **CNN learns 32 different filters (kernels)**, each of size **3×3**. These filters extract different features from the input image.\n",
    "\n",
    "---\n",
    "\n",
    "### **What Do the Filters Look Like?**  \n",
    "Each filter is a small **3×3 matrix** with values that the CNN **learns** during training. The values in the filters are initialized randomly and updated via backpropagation.\n",
    "\n",
    "#### **Example: Filters Learned in a CNN**\n",
    "If we visualize some of the **32 learned filters**, they may look like:\n",
    "\n",
    "```\n",
    "Filter 1 (Edge Detector)\n",
    "[-1  -1  -1]\n",
    "[ 0   0   0]\n",
    "[ 1   1   1]\n",
    "\n",
    "Filter 2 (Blur)\n",
    "[1  1  1]\n",
    "[1  1  1]\n",
    "[1  1  1]\n",
    "\n",
    "Filter 3 (Sharpen)\n",
    "[ 0  -1   0]\n",
    "[-1   5  -1]\n",
    "[ 0  -1   0]\n",
    "\n",
    "Filter 4 (Vertical Edge Detector)\n",
    "[-1   0   1]\n",
    "[-1   0   1]\n",
    "[-1   0   1]\n",
    "\n",
    "...  (28 more filters)\n",
    "```\n",
    "\n",
    "Each filter extracts different **patterns** (edges, textures, shapes, etc.). When 32 filters are applied, the output will have **32 feature maps**.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to View the Filters in a Trained CNN?**\n",
    "If you train a CNN and want to **see** the learned filters:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a simple CNN\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1))\n",
    "])\n",
    "\n",
    "# Extract filters (weights)\n",
    "filters, biases = model.layers[0].get_weights()\n",
    "\n",
    "# Normalize values for visualization\n",
    "filters_min = filters.min()\n",
    "filters_max = filters.max()\n",
    "filters = (filters - filters_min) / (filters_max - filters_min)\n",
    "\n",
    "# Plot some filters\n",
    "fig, axes = plt.subplots(4, 8, figsize=(10, 5))\n",
    "for i in range(32):  # 32 filters\n",
    "    ax = axes[i // 8, i % 8]\n",
    "    ax.imshow(filters[:, :, 0, i], cmap='gray')  # Show as grayscale\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- `Conv2D(32, (3,3), ...)` learns **32 filters**, each detecting different features.  \n",
    "- These filters evolve during training through **backpropagation**.  \n",
    "- We can **visualize them** using `get_weights()`.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing CNN Filters Using a Real Image**\n",
    "Now, let's train a simple CNN on the **MNIST dataset** (handwritten digits) and visualize the **32 learned filters** from the first convolutional layer.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: Load and Preprocess the MNIST Dataset**\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset (handwritten digits 0-9)\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the images (convert pixel values from 0-255 to 0-1)\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Reshape to add a single channel (grayscale images)\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "```\n",
    "---\n",
    "\n",
    "## **Step 2: Define and Train a CNN Model**\n",
    "```python\n",
    "# Define a CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)), # 32 filters of 3x3\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),  \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')  # 10 output classes (digits 0-9)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on MNIST (using a subset for quick training)\n",
    "model.fit(X_train[:10000], y_train[:10000], epochs=5, batch_size=64, validation_data=(X_test[:2000], y_test[:2000]))\n",
    "```\n",
    "---\n",
    "\n",
    "## **Step 3: Extract and Visualize the Filters**\n",
    "```python\n",
    "# Get the first Conv2D layer\n",
    "conv_layer = model.layers[0]  # First convolutional layer\n",
    "\n",
    "# Get the filters (weights)\n",
    "filters, biases = conv_layer.get_weights()\n",
    "\n",
    "# Normalize filter values for visualization\n",
    "filters_min = filters.min()\n",
    "filters_max = filters.max()\n",
    "filters = (filters - filters_min) / (filters_max - filters_min)\n",
    "\n",
    "# Plot all 32 filters\n",
    "fig, axes = plt.subplots(4, 8, figsize=(12, 6))\n",
    "for i in range(32):  # 32 filters\n",
    "    ax = axes[i // 8, i % 8]\n",
    "    ax.imshow(filters[:, :, 0, i], cmap='gray')  # Show as grayscale\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Learned Filters (Kernels) from First Conv2D Layer\", fontsize=14)\n",
    "plt.show()\n",
    "```\n",
    "---\n",
    "\n",
    "## **Step 4: Apply Filters to a Real Image**\n",
    "Let's pick an image from MNIST and see how each filter **transforms** it.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Choose an image from test set\n",
    "image = X_test[5].reshape(1, 28, 28, 1)\n",
    "\n",
    "# Create a model that outputs the feature maps after the first Conv2D layer\n",
    "layer_output_model = Model(inputs=model.input, outputs=conv_layer.output)\n",
    "\n",
    "# Get the feature maps\n",
    "feature_maps = layer_output_model.predict(image)\n",
    "\n",
    "# Plot the 32 feature maps (outputs of the 32 filters)\n",
    "fig, axes = plt.subplots(4, 8, figsize=(12, 6))\n",
    "for i in range(32):\n",
    "    ax = axes[i // 8, i % 8]\n",
    "    ax.imshow(feature_maps[0, :, :, i], cmap='gray')  # Feature map output\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Feature Maps After First Conv2D Layer\", fontsize=14)\n",
    "plt.show()\n",
    "```\n",
    "---\n",
    "\n",
    "### **What We Have Done:**\n",
    "1. **Loaded and preprocessed** the MNIST dataset.  \n",
    "2. **Trained a CNN model** with 32 filters in the first convolutional layer.  \n",
    "3. **Extracted and visualized the learned filters** (3×3 matrices).  \n",
    "4. **Applied the filters** to a real image and displayed the resulting feature maps.  \n",
    "\n",
    "This shows how CNNs **extract edges and patterns** using learned filters!  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_int",
   "language": "python",
   "name": "ml_int"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
